{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc:\n",
    "web_base = 'https://data.sandiego.gov/datasets/?department=police'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What we want to keep as a general outline for data\n",
    "\n",
    "stop_id\n",
    "\n",
    "stop_cause / reason_for_stop\n",
    "\n",
    "result_of_stop\n",
    "\n",
    "stopdate\n",
    "\n",
    "stoptime\n",
    "\n",
    "service_area\n",
    "\n",
    "block\n",
    "\n",
    "subject_race / race --> map to race codes\n",
    "\n",
    "beat (new) --> map to service area\n",
    "\n",
    "actions_taken (new)\n",
    "\n",
    "officer_assignment_key (new)\n",
    "\n",
    "exp_years (new)\n",
    "\n",
    "stopduration (new)\n",
    "\n",
    "perceived_age (new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_table(yr):\n",
    "    if yr < 2018:\n",
    "        url = 'http://seshat.datasd.org/pd/vehicle_stops_{}_datasd_v1.csv'.format(yr)\n",
    "        return pd.read_csv(url)\n",
    "    \n",
    "    else: # post- 2018\n",
    "        # cols we care about\n",
    "        cols_2018 = ['stop_id', 'date_stop', 'time_stop', 'stopduration', \n",
    "                 'officer_assignment_key', 'exp_years', 'beat', \n",
    "                 'perceived_age', 'gend']\n",
    "\n",
    "        url = 'http://seshat.datasd.org/pd/ripa_stops_datasd_v1.csv'\n",
    "        df = pd.read_csv(url)\n",
    "        return df[cols_2018]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns we still need:\n",
    "\n",
    "stop_cause / reason_for_stop\n",
    "\n",
    "result_of_stop\n",
    "\n",
    "actions_taken "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# other tables for POST- 2018\n",
    "def get_merge_data():\n",
    "    reason = 'stop_reason'\n",
    "    reason_cols = ['stop_id', 'reason_for_stop', 'reason_for_stopcode']\n",
    "\n",
    "    result = 'stop_result'\n",
    "    result_cols= ['stop_id', 'result']\n",
    "\n",
    "    race = 'race'\n",
    "    race_cols = ['stop_id', 'race']\n",
    "\n",
    "    action = 'actions_taken'\n",
    "    action_cols = ['stop_id', 'action']\n",
    "\n",
    "    base = 'http://seshat.datasd.org/pd/ripa_{}_datasd.csv'\n",
    "\n",
    "    reason_df = pd.read_csv(base.format(reason))\n",
    "    result_df = pd.read_csv(base.format(result))\n",
    "    race_df = pd.read_csv(base.format(race))\n",
    "    action_df = pd.read_csv(base.format(action))\n",
    "    \n",
    "    return [[reason_cols, result_cols, race_cols, action_cols], [reason_df, result_df, race_df, action_df]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the new cols from other urls\n",
    "def gen_cols(df1, gen_df, cols):\n",
    "    new = df1.merge(gen_df, on = 'stop_id')\n",
    "    drop = [x for x in new.columns if x not in cols]\n",
    "    new = new.drop(columns = drop).drop_duplicates(subset = 'stop_id')\n",
    "    return new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_data(tbl):\n",
    "    merge_cols = get_merge_data[0]\n",
    "    merge_dfs = get_merge_data[1]\n",
    "    merged = []\n",
    "\n",
    "    for i in range(len(merge_dfs)):\n",
    "        merged.append(gen_cols(tbl, merge_dfs[i], merge_cols[i]))\n",
    "          \n",
    "    first = merged[0]\n",
    "    for i in range(0, len(merged)):\n",
    "        if i == len(merged) - 1:\n",
    "            break \n",
    "        first = pd.merge(first, merged[i + 1], on = 'stop_id')\n",
    "\n",
    "    return pd.merge(basic, first, on='stop_id').drop_duplicates(subset = 'stop_id')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean column names\n",
    "\n",
    "* These are the same: 'stop_id', 'date_stop', 'time_stop', 'service_area', 'subject_race'/'Race Code', 'subject_sex'/'gend',\n",
    "\n",
    "\n",
    "* These are excess for post- 2018: 'beat', 'officer_assignment_key', 'exp_years', 'perceived_age'\n",
    "\n",
    "\n",
    "* These are excess for pre- 2018: \n",
    "    - 'sd_resident'\n",
    "    - 'arrested'\n",
    "    - 'searched'\n",
    "    - 'obtained_consent'\n",
    "    - 'contraband_found'\n",
    "    - 'property_seized'\n",
    "\n",
    "\n",
    "&rightarrow;  Therefore: Need to make 'subject_race' == 'Race Code' and  'subject_sex' == 'gend'\n",
    "\n",
    "\n",
    "* Additionally for pre- 2018, need to engineer: 'outcome'\n",
    "\n",
    "    -  from ['arrested', 'searched', 'property_seized'] in pre- 2018 and ['action', 'result'] in post- 2018\n",
    "\n",
    "\n",
    "* Columns we are ok with having lots of null values because they are useful for analysis:\n",
    "\n",
    "    - 'sd_resident'\n",
    "    - 'beat'\n",
    "    - 'officer_assignment_key'\n",
    "    - 'exp_years'\n",
    "    - 'perceived_age'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for PRE- 2018\n",
    "def change_bool(string):\n",
    "        if (string == 'Y') | (string =='y'):\n",
    "            return 1\n",
    "        if (string == 'N') | (string == 'n'):\n",
    "            return 0\n",
    "        return np.nan\n",
    "\n",
    "def map_bool(cols, df):\n",
    "    for col in cols:\n",
    "        if col not in list(df.columns):\n",
    "            continue\n",
    "        df[col] = df[col].apply(lambda x: c_bool(x))\n",
    "    return df\n",
    "\n",
    "# only for POST- 2018\n",
    "def change_sex(x):\n",
    "    if (x == 1.0) | (x == 1):\n",
    "        return 'M'\n",
    "    elif (x == 2.0) | (x == 2):\n",
    "        return 'F'\n",
    "    else:\n",
    "        return np.nan\n",
    "\n",
    "def map_sex(col):\n",
    "    return col.apply(lambda x: change_sex(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accounts for both formats\n",
    "def clean_bool(df, yr):\n",
    "    if yr < 2018:\n",
    "        c = ['sd_resident', 'searched', 'contraband_found', 'property_seized', 'arrested']\n",
    "        return map_bool(c, df)\n",
    "    \n",
    "    else:\n",
    "        df['gend'] = map_sex(df['gend'])\n",
    "        return df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### columns we still need to map:\n",
    "\n",
    "subject_race / race - map to race codes\n",
    "\n",
    "beat - map to service area"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only for POST- 2018\n",
    "def map_race(df):\n",
    "    # engineered from existing file\n",
    "    # race_codes = 'http://seshat.datasd.org/pd/vehicle_stops_race_codes.csv'\n",
    "    # and intution\n",
    "\n",
    "    race_dict = {\n",
    "     'Asian' : 'A',\n",
    "     'OTHER ASIAN': 'A',\n",
    "     'Middle Eastern or South Asian': 'M',\n",
    "     'BLACK': 'B',\n",
    "     'CHINESE': 'C',\n",
    "     'CAMBODIAN': 'D',\n",
    "     'FILIPINO': 'F',\n",
    "     'GUAMANIAN': 'G',\n",
    "     'HISPANIC': 'H',\n",
    "     'Hispanic/Latino/a': 'H',\n",
    "     'INDIAN': 'I',\n",
    "     'JAPANESE': 'J',\n",
    "     'KOREAN': 'K',\n",
    "     'LAOTIAN': 'L',\n",
    "     'OTHER': 'O',\n",
    "     'PACIFIC ISLANDER': 'P',\n",
    "     'Pacific Islander': 'P',\n",
    "     'SAMOAN': 'S',\n",
    "     'HAWAIIAN': 'U',\n",
    "     'VIETNAMESE': 'V',\n",
    "     'WHITE': 'W',\n",
    "     'White': 'W',\n",
    "     'ASIAN INDIAN': 'Z',\n",
    "     'Native American': 'N'\n",
    "    }\n",
    "    df['Race Code'] = df['race'].map(race_dict)\n",
    "    return df\n",
    "\n",
    "# only for POST- 2018\n",
    "def map_service_area(df):\n",
    "    stop_beats = 'http://seshat.datasd.org/sde/pd/pd_beats_datasd.geojson'\n",
    "    beats = gpd.read_file(stop_beats)\n",
    "    # get unique beats\n",
    "    unique_beats = beats[['beat', 'serv']].drop_duplicates('beat')\n",
    "    beat_dict = dict(zip(unique_beats.beat, uni.serv))\n",
    "    df['service_area'] = ['beat'].map(beat_dict)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rename_cols(df):\n",
    "    return df.rename(columns={'Race Code': 'subject_race', 'gend': 'subject_sex', 'reason_for_stop': 'stop_cause'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Outcome possible values: \n",
    "\n",
    "# 'None'\n",
    "# 'Warning (verbal or written)' \n",
    "# 'Search of property was conducted' \n",
    "# 'Property was seized' >> property seized\n",
    "# 'Custodial Arrest without warrant'\n",
    "\n",
    "def check_outcome(row, year):\n",
    "    if year < 2018:\n",
    "        # check 'arrested', 'searched', 'property_seized'\n",
    "        if row.arrested == 1.0:\n",
    "            return 'Arrest'\n",
    "        elif row.property_seized == 1.0:\n",
    "            return 'Property was seized'\n",
    "        elif row.searched == 1.0:\n",
    "            return 'Search of property was conducted'\n",
    "        return 'Not Applicable'\n",
    "        \n",
    "    else:\n",
    "        a = row.action\n",
    "        r = row.result\n",
    "        \n",
    "        # check worst outcome first\n",
    "        if r == 'Custodial Arrest without warrant':\n",
    "            return r\n",
    "        elif (a == 'Search of property was conducted') | (a == 'Property was seized'):\n",
    "            return a\n",
    "        elif r == 'Warning (verbal or written)':\n",
    "            return r\n",
    "        \n",
    "        return 'Not Applicable'\n",
    "    \n",
    "def outcome_map(df, yr):\n",
    "    df_copy = df.copy()\n",
    "    df_copy['Outcome'] = df_copy.apply(lambda x: check_outcome(x, yr), axis=1)\n",
    "    \n",
    "    if yr < 2018:\n",
    "        return df_copy.drop(columns=['arrested', 'property_seized', 'searched', 'obtained_consent', 'contraband_found'])\n",
    "        \n",
    "    return df_copy.drop(columns=['action', 'result'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pre_2018_format(df, yr):\n",
    "    df = clean_bool(df, yr)\n",
    "    df = outcome_map(df, yr)\n",
    "    return df\n",
    "\n",
    "def post_2018_format(df, yr):\n",
    "    df = clean_bool(df, yr)\n",
    "    df = map_race(df)\n",
    "    df = map_service_area(df)\n",
    "    df = rename_cols(df)\n",
    "    df = outcome_map(df, yr)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_df(df, yr):\n",
    "    order = ['stop_id', 'stop_cause', 'reason_for_stopcode', 'date_stop', 'time_stop', 'stopduration', 'stop_cause', \n",
    "         'Outcome', 'beat', 'service_area', 'subject_race', 'subject_sex', 'perceived_age', 'sd_resident', \n",
    "         'officer_assignment_key', 'exp_years']\n",
    "    \n",
    "    cols = [i for i in order if i in list(df.columns)]\n",
    "    if yr < 2018:\n",
    "        return pre_2018_format(df, yr)[cols]\n",
    "    \n",
    "    else:\n",
    "        return post_2018_format(df, yr)[cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data(yr):\n",
    "    tbl = get_table(yr)\n",
    "    \n",
    "    if yr < 2018:\n",
    "        return format_df(tbl, yr)\n",
    "\n",
    "    else:\n",
    "        df = merge_data(tbl)\n",
    "        return format_df(df, yr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-36-02139929772d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m2018\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-35-7cabb0a83b9c>\u001b[0m in \u001b[0;36mget_data\u001b[1;34m(yr)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mget_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[0mtbl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_table\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0myr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0myr\u001b[0m \u001b[1;33m<\u001b[0m \u001b[1;36m2018\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mformat_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtbl\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0myr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-24-741881f6bf2c>\u001b[0m in \u001b[0;36mget_table\u001b[1;34m(yr)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m         \u001b[0murl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'http://seshat.datasd.org/pd/ripa_stops_datasd_v1.csv'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m         \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0murl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcols_2018\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[0;32m    683\u001b[0m         )\n\u001b[0;32m    684\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 685\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    686\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    687\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\parsers.py\u001b[0m in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    438\u001b[0m     \u001b[1;31m# See https://github.com/python/mypy/issues/1297\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    439\u001b[0m     fp_or_buf, _, compression, should_close = get_filepath_or_buffer(\n\u001b[1;32m--> 440\u001b[1;33m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    441\u001b[0m     )\n\u001b[0;32m    442\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"compression\"\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode)\u001b[0m\n\u001b[0;32m    199\u001b[0m             \u001b[1;31m# Override compression based on Content-Encoding header\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m             \u001b[0mcompression\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"gzip\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m         \u001b[0mreader\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBytesIO\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m         \u001b[0mreq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mreader\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcompression\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36mread\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    468\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    469\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 470\u001b[1;33m                     \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_safe_read\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlength\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    471\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_close_conn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\http\\client.py\u001b[0m in \u001b[0;36m_safe_read\u001b[1;34m(self, amt)\u001b[0m\n\u001b[0;32m    618\u001b[0m         \u001b[0ms\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    619\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[0mamt\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 620\u001b[1;33m             \u001b[0mchunk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mamt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMAXAMOUNT\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    621\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mchunk\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    622\u001b[0m                 \u001b[1;32mraise\u001b[0m \u001b[0mIncompleteRead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mb''\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ms\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mamt\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[1;34m(self, b)\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[1;32mwhile\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    588\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 589\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    590\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    591\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "get_data(2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning\n",
    "Perform an initial EDA to statistically assess the quality of the data and its appropriateness for addressing the problem at hand, justifying data cleaning logic. This will likely address issues with accuracy, precision, and missingness of specific attributes, tying these issues to their possible impact over eventual results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Descriptive Stats\n",
    "Statistically summarize the relevant, cleaned attributes and derived features (e.g. in univariate and bivariate analyses) for San Diego."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Traffic Stop Analysis\n",
    "Calculate and document the differences in stop rates and post-stop outcomes. The analysis should address possible reasons for such differences (including addressing possible confounders). Additionally: - The significance of these differences should be tested using statistical inference. - These differences should also be calculated across other variables of interest (e.g. service area)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Veil of Darkness\n",
    "Perform the Veil of Darkness analysis for San Diego. Include an introduction to the technique and interpret the results. (In Assignment #3 you will carefully address the shortcoming of this result.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Develop code to clean data (as defined and justified in Part 1), create the features for the replication, and compute the statistics for the report. Such code should conform to the methodology portion of the course (e.g. using the project template).\n",
    "\n",
    "In particular, your project should have a run.py with the following targets:\n",
    "\n",
    "* data creates the data needed for analysis.\n",
    "* process cleans and prepares the data for analysis (e.g. cleaning and feature creation).\n",
    "* data-test ingests a small amount of test data (that process can then process)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
